{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11548,"status":"ok","timestamp":1727422208926,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"zoFKPNPGKqY1","outputId":"43f83881-dee3-4d90-bb7e-1b543610fbab"},"outputs":[],"source":["# Install FlyGym. This should take about 1 minute.\n","!pip install \"git+https://github.com/NeLy-EPFL/flygym.git@dev-v1.0.2-pre.1\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":703,"status":"ok","timestamp":1727422209626,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"vodzMdCxKqY3","outputId":"54146476-c28c-4e48-c8bb-8669ce88f24a"},"outputs":[],"source":["# Set up GPU a few more and rendering parameters. This should take ~1 second.\n","\n","from google.colab import files\n","\n","import distutils.util\n","import os\n","import subprocess\n","\n","if subprocess.run(\"nvidia-smi\").returncode:\n","    raise RuntimeError(\n","        \"Cannot communicate with GPU. \"\n","        \"Make sure you are using a GPU Colab runtime. \"\n","        \"Go to the Runtime menu and select Choose runtime type.\"\n","    )\n","\n","# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n","# This is usually installed as part of an Nvidia driver package, but the Colab\n","# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n","# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n","NVIDIA_ICD_CONFIG_PATH = \"/usr/share/glvnd/egl_vendor.d/10_nvidia.json\"\n","if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n","    with open(NVIDIA_ICD_CONFIG_PATH, \"w\") as f:\n","        f.write(\n","            \"\"\"{\n","    \"file_format_version\" : \"1.0.0\",\n","    \"ICD\" : {\n","        \"library_path\" : \"libEGL_nvidia.so.0\"\n","    }\n","}\n","\"\"\"\n","        )\n","\n","# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n","print(\"Setting environment variable to use GPU rendering:\")\n","%env MUJOCO_GL=egl\n","\n","try:\n","    print(\"Checking that the installation succeeded:\")\n","    import mujoco\n","\n","    mujoco.MjModel.from_xml_string(\"<mujoco/>\")\n","except Exception as e:\n","    raise e from RuntimeError(\n","        \"Something went wrong during installation. Check the shell output above \"\n","        \"for more information.\\n\"\n","        \"If using a hosted Colab runtime, make sure you enable GPU acceleration \"\n","        'by going to the Runtime menu and selecting \"Choose runtime type\".'\n","    )\n","\n","print(\"Installation successful.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1727422209626,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"YCPSFdLrKqY3","outputId":"9329d778-5b33-4c07-c2a8-df3e462abcc3"},"outputs":[],"source":["# clone the repo to colab to get the data\n","! git clone \"https://github.com/NeLy-EPFL/neuromechfly-workshop.git\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1727422209626,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"_XTyfe4EMvqh","outputId":"8331f48b-7606-47d1-c31f-7557255f9d6a"},"outputs":[],"source":["!ls neuromechfly-workshop/kinematic_replay/data/video_data/"]},{"cell_type":"markdown","metadata":{"id":"KVk7rLMCKqY4"},"source":["# Example: Kinematic replay of experimentally recorded behavior\n","\n","We now move on to an example where we kinematically replay some experimentally recorded walking behaviors. Specifically, we recorded an untethered fly walking in a narrow corrdior. We used the GUI of SLEAP to manually annotate ~50 frames of behavior recorded at 360 fps and downsampled to 130fps. When then leveraged the simple geometrical constraints of our setup to triangulate the 2d poses to a 3d poses. Used df3d post processing for alignement and interpolation and finally used Seqikpy to perform inverse kinematics.\n","\n","We are replaying this exact behavior:\n","\n","<video width=\"\" height=\"\" controls>\n","  <source src=\"neuromechfly-workshop/kinematic_replay/data/video_data/straight_walking_bout.mp4\" type=\"video/mp4\">\n","</video>\n","\n","All the notebooks used to go from a raw video to joint angles can be found in this repo in the folder: 2d-3d\n","\n","With these we will use a PD controller to actuate the DoFs of the simulated fly at using these exact angles to see if the fly can walk untethered on flat terrain, as shown in the original NeuroMechFly paper (Lobato-Rios et al., *Nature Methods* 2022).\n","\n","We start with the necessary imports:"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2133,"status":"ok","timestamp":1727422211757,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"YxqMKcCbKqY5"},"outputs":[],"source":["import numpy as np\n","import pickle\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","from tqdm import trange\n","\n","from flygym import Fly, Camera, SingleFlySimulation, get_data_path\n","from flygym.preprogrammed import all_leg_dofs, all_tarsi_links\n","\n","from IPython.display import Video\n","from copy import deepcopy"]},{"cell_type":"markdown","metadata":{"id":"X-wIbkacKqY5"},"source":["# Load and format the raw data"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1727422211758,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"-bA9jfYnKqY5"},"outputs":[],"source":["def format_seqikpy_data(data, corresp_dict={\"ThC\": \"Coxa\",\n","                                             \"CTr\": \"Femur\",\n","                                               \"FTi\": \"Tibia\",\n","                                                 \"TiTa\": \"Tarsus1\"}\n","                                                 ):\n","\n","    data_gym = {}\n","    for joint, values in data.items():\n","        if joint == \"meta\" or joint == \"swing_stance_time\":\n","            data_gym[joint] = values\n","        else:\n","            leg = joint[6:8]\n","            joint_name = joint[9:]\n","            seg, dof = joint_name.split(\"_\")\n","            if dof == \"pitch\":\n","                newjoint = f\"joint_{leg}{corresp_dict[seg]}\"\n","            else:\n","                newjoint = f\"joint_{leg}{corresp_dict[seg]}_{dof}\"\n","\n","            data_gym[newjoint] = values\n","\n","\n","    return data_gym\n","\n","seq_ikpy_data_path = Path(\"neuromechfly-workshop/kinematic_replay/data/inverse_kinematics/leg_joint_angles.pkl\")\n","with open(seq_ikpy_data_path, \"rb\") as f:\n","    seq_ikdata = pickle.load(f)\n","\n","data = format_seqikpy_data(seq_ikdata)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1727422211758,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"SLa1LPKYKqY5","outputId":"d09e2cee-7dc7-48a9-9dc6-08463e9ef622"},"outputs":[],"source":["data[\"meta\"].keys()"]},{"cell_type":"markdown","metadata":{"id":"0dIg96qEKqY6"},"source":["# Set hyperparameters"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1727422211758,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"mtW_L1EoKqY6"},"outputs":[],"source":["timestep = 1e-4\n","actuated_joints = all_leg_dofs\n","run_time = len(data[\"joint_RFCoxa_yaw\"])*data[\"meta\"][\"timestep\"]\n","\n","output_dir = Path(\"outputs\")\n","output_dir.mkdir(exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1727422211758,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"6GtLA-juKqY6","outputId":"addab902-a761-491d-83eb-2738e880148e"},"outputs":[],"source":["target_num_steps = int(run_time / timestep)\n","data_block = np.zeros((len(actuated_joints), target_num_steps))\n","input_t = np.arange(len(data[\"joint_LFCoxa\"])) * data[\"meta\"][\"timestep\"]\n","output_t = np.arange(target_num_steps) * timestep\n","for i, joint in enumerate(actuated_joints):\n","    data_block[i, :] = np.interp(output_t, input_t, data[joint])\n","\n","print(\"Neuromechfly has {} actuated joints and the data contains {} interpolated steps adding up to a toal of {} seconds\".format(*data_block.shape, run_time))"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1727422211758,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"HTGHF1jAKqY6"},"outputs":[],"source":["#Â The fly should walk more on the tippy toes\n","tarsus_offset = np.zeros(len(actuated_joints))\n","for i, joint in enumerate(actuated_joints):\n","    if \"Tarsus\" in joint:\n","        data_block[i, :] = -1*np.pi/5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":606},"executionInfo":{"elapsed":2686,"status":"ok","timestamp":1727422214442,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"zPWzLYgYKqY6","outputId":"e6012b41-46c4-4a7d-8146-88a8d4dbebff"},"outputs":[],"source":["fig, axs = plt.subplots(\n","    3, 2, figsize=(8, 6), sharex=True, sharey=True, tight_layout=True\n",")\n","legs = [\n","    f\"{side} {pos} leg\"\n","    for pos in [\"front\", \"middle\", \"hind\"]\n","    for side in [\"Left\", \"Right\"]\n","]\n","for i, leg in enumerate(legs):\n","    ax = axs.flatten()[i]\n","    leg_code = f\"{leg.split()[0][0]}{leg.split()[1][0]}\".upper()\n","    for j, dof in enumerate(actuated_joints):\n","        if dof.split(\"_\")[1][:2] != leg_code:\n","            continue\n","        ax.plot(output_t, np.rad2deg(data_block[j, :]), label=dof[8:])\n","    ax.set_ylim(-180, 180)\n","    ax.set_xlabel(\"Time (s)\")\n","    ax.set_ylabel(\"Angle (degree)\")\n","    ax.set_yticks([-180, -90, 0, 90, 180])\n","    ax.set_title(leg)\n","    if leg == \"Right front leg\":\n","        ax.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\", borderaxespad=0)\n","\n","fig.savefig(output_dir / \"single_step.png\")"]},{"cell_type":"markdown","metadata":{"id":"DRAjF3qfKqY6"},"source":["In every leg 7 degrees of freedom are actuated adding up to 6*7 actuated joints."]},{"cell_type":"markdown","metadata":{"id":"Qk-haWKeKqY7"},"source":["# Determine swing and stance initiation"]},{"cell_type":"markdown","metadata":{"id":"FDhYk_q0KqY7"},"source":["In this example swing and stance initiation have been annotated manually from the video. For convinience, we compute when in seconds, the swing and the stance starts to accordingly actuate the adhesion."]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1727422214442,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"uxlx_fA4KqY7"},"outputs":[],"source":["legs = [side+pos for side in \"LR\" for pos in \"FMH\"]\n","# swing_stance_intervals = {}\n","\n","# for leg in legs:\n","#     swing_stance_intervals[leg] = []\n","#     stance_starts = data[\"swing_stance_time\"][leg][\"stance\"]\n","#     stance_ends = data[\"swing_stance_time\"][leg][\"swing\"]\n","\n","#     for start in stance_starts:\n","#         selected_end = None\n","#         for end in stance_ends:\n","#             if end > start:\n","#                 selected_end = end\n","#                 break\n","#         if selected_end is not None:\n","#             swing_stance_intervals[leg].append((start, selected_end))\n","#         else:\n","#             swing_stance_intervals[leg].append((start, run_time))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27708,"status":"ok","timestamp":1727422437004,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"69AnrcA3KqY7","outputId":"5464502a-9251-412c-b9cf-2383a6df2394"},"outputs":[],"source":["tarsal_joints = [\"_\".join([\"joint\", tarsus_seg]) for tarsus_seg in all_tarsi_links if not \"Tarsus1\" in tarsus_seg]\n","monitored_joints = actuated_joints + tarsal_joints\n","fly = Fly(init_pose=\"stretch\", actuated_joints=actuated_joints, control=\"position\", enable_adhesion=True, draw_adhesion=True, monitored_joints=monitored_joints, enable_vision=True, render_raw_vision=True)\n","play_speed = 0.05\n","cam = Camera(fly=fly, camera_id=\"Animat/camera_left\", play_speed=play_speed, draw_contacts=True, play_speed_text=True)\n","sim = SingleFlySimulation(\n","    fly=fly,\n","    cameras=[cam],\n",")\n","obs, info = sim.reset()\n","obs_list = []\n","raw_vision_list = []\n","\n","n_stabilization_steps = 1000\n","for i in trange(n_stabilization_steps):\n","    action = {\"joints\": data_block[:, 0], \"adhesion\": np.zeros(6)}\n","    obs, reward, terminated, truncated, info = sim.step(action)\n","\n","#reset the current time to exclude the stabilization steps\n","sim.curr_time = 0\n","render_times = []\n","\n","\n","for i in trange(target_num_steps):\n","    # here, we simply use the recorded joint angles as the target joint angles\n","    joint_pos = data_block[:, i]\n","    adhesion = np.zeros(6)\n","    # for i, leg in enumerate(legs):\n","    #     for start, end in swing_stance_intervals[leg]:\n","    #         if start <= sim.curr_time  <= end:\n","    #             adhesion[i] = 1\n","    #             break\n","    action = {\"joints\": joint_pos, \"adhesion\": adhesion}\n","    obs, reward, terminated, truncated, info = sim.step(action)\n","    obs_list.append(obs.copy())\n","    rend_flag = sim.render()[0]\n","    if not rend_flag is None:\n","        render_times.append(sim.curr_time)\n","\n","    #if fly.render_raw_vision:\n","        #raw_vision_list.append(info[\"raw_vision\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171,"resources":{"http://localhost:8080/outputs/kinematic_replay_camera_left_contacts.mp4":{"data":"","headers":[["content-length","0"]],"ok":false,"status":404,"status_text":""}}},"executionInfo":{"elapsed":2668,"status":"ok","timestamp":1727422449606,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"veoqmOquKqY7","outputId":"e96472ad-39a4-4def-81ec-5f338bb25982"},"outputs":[],"source":["video_name = \"kinematic_replay_{}.mp4\".format(cam.camera_id.split(\"/\")[1])\n","if cam.draw_contacts:\n","    video_name = video_name.replace(\".mp4\", \"_contacts.mp4\")\n","\n","cam.save_video(output_dir / video_name, stabilization_time=0)\n","np.savetxt(output_dir / \"render_times.txt\", render_times)\n","\n","# show the video\n","Video(output_dir/ video_name)"]},{"cell_type":"markdown","metadata":{"id":"NQx2ouCWKqY7"},"source":["The first output of kinematic replay is a video that can be obtained from any different angles. Arrows at the leg tips represent the contact forces with the floor and the color of the tarsal segments encode the presence of absence of adhesion. Blue means adhesion is turned ON and adhering to the floor. Red means adhesion is ON but the leg is not in contact with the floor and finally if the tarsus is in the base color it means adhesion is OFF"]},{"cell_type":"markdown","metadata":{"id":"BrzH6_cSKqY7"},"source":["# Leveraging kinematic replay"]},{"cell_type":"markdown","metadata":{"id":"Y5Fk9uj8KqY7"},"source":["## Observation space"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1727422449606,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"nnJ6-TQSKqY7","outputId":"82d28890-aafc-41be-f153-1c02757d6704"},"outputs":[],"source":["print(f\"Observation list made {len(obs_list)} observations\")\n","print(\"One observation contains information about:\")\n","for key in obs_list[0].keys():\n","    try:\n","        print(key, obs[key].shape)\n","    except AttributeError:\n","        print(key)"]},{"cell_type":"markdown","metadata":{"id":"u82QUvXNKqY7"},"source":["## Joint torques"]},{"cell_type":"markdown","metadata":{"id":"6fGeWQcgKqY7"},"source":["Torque within joints would be impossible to monitor in a behaving fly. Being able to infer them from kinematic replay allows us to get an abstract sensory signal that could be extracted from the fly's sensory organs. With more modelling efforts one could directly implement sensory organs that would change their firing rate depending on the experienced torque."]},{"cell_type":"markdown","metadata":{"id":"jnzg-qD1KqY7"},"source":["### Torque in actively controlled joints"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":911},"executionInfo":{"elapsed":2403,"status":"ok","timestamp":1727422453585,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"Vbu3YXVlKqY7","outputId":"90d9a423-351e-46aa-be91-a3ec5f70f82d"},"outputs":[],"source":["leg_joints_to_id = {leg: [i for i, joint in enumerate(monitored_joints) if leg in joint and not \"Tarsus\" in joint] for leg in legs}\n","leg_torques = np.array([[obs[\"joints\"][2, leg_joints_to_id[leg]] for obs in obs_list] for leg in legs])\n","leg_joints = {leg: [joint for joint in monitored_joints if leg in joint and not \"Tarsus\" in joint] for leg in legs}\n","\n","time = np.arange(len(leg_torques[0])) * timestep\n","\n","fig, axs = plt.subplots(\n","    3, 2, figsize=(20, 12), tight_layout=True\n",")\n","\n","for i, leg in enumerate(legs):\n","    ax = axs.flatten()[i]\n","    for j, joint in enumerate(leg_joints[leg]):\n","\n","        ax.plot(output_t, leg_torques[i, :, j], label=joint)\n","    ax.set_xlabel(\"Time (s)\")\n","    ax.set_ylabel(\"Torque (Nm)\")\n","    ax.set_title(leg)\n","    if leg == \"LF\":\n","        ax.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\", borderaxespad=0)\n"]},{"cell_type":"markdown","metadata":{"id":"zRHhKjc3KqY8"},"source":["### Torque in passive joints (Tarsal joints)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":911},"executionInfo":{"elapsed":5824,"status":"ok","timestamp":1727422270090,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"Q3ekKY0nKqY8","outputId":"c7a1a0cf-0ede-44c5-ef50-4f49bf25992d"},"outputs":[],"source":["leg_joints_to_id = {leg: [i for i, joint in enumerate(monitored_joints) if leg in joint and \"Tarsus\" in joint] for leg in legs}\n","leg_torques = np.array([[obs[\"joints\"][2, leg_joints_to_id[leg]] for obs in obs_list] for leg in legs])\n","leg_joints = {leg: [joint for joint in monitored_joints if leg in joint and \"Tarsus\" in joint] for leg in legs}\n","\n","time = np.arange(len(leg_torques[0])) * timestep\n","\n","fig, axs = plt.subplots(\n","    3, 2, figsize=(20, 12), tight_layout=True\n",")\n","\n","for i, leg in enumerate(legs):\n","    ax = axs.flatten()[i]\n","    for j, joint in enumerate(leg_joints[leg]):\n","\n","        ax.plot(output_t, leg_torques[i, :, j], label=joint)\n","    ax.set_xlabel(\"Time (s)\")\n","    ax.set_ylabel(\"Torque (Nm)\")\n","    ax.set_title(leg)\n","    if leg == \"LF\":\n","        ax.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\", borderaxespad=0)\n"]},{"cell_type":"markdown","metadata":{"id":"FfqvDD_XKqY8"},"source":["## Floor contact force"]},{"cell_type":"markdown","metadata":{"id":"DomLT4zZKqY8"},"source":["While surely contributing to adaptative motor control ground reaction forces are really hard to measure in behaving *Drosophila* melanogaster. Using Neuromechfly it is possible to infer those forces and study their contribution to adaptative motor control."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":4791,"status":"ok","timestamp":1727422274878,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"n1NSpNeTKqY8","outputId":"01964257-74b6-492b-f641-bf7910a5a312"},"outputs":[],"source":["leg_tarsal_seg_contact_id = {leg: [i for i, tarsal_seg in enumerate(all_tarsi_links) if leg in tarsal_seg] for leg in legs}\n","leg_contacts = np.array([[np.linalg.norm(obs[\"contact_forces\"][leg_tarsal_seg_contact_id[leg], :], axis=1) for obs in obs_list] for leg in legs])\n","leg_contact_tarsalseg = {leg: [tarsal_seg for tarsal_seg in all_tarsi_links if leg in tarsal_seg] for leg in legs}\n","\n","time = np.arange(len(leg_torques[0])) * timestep\n","\n","fig, axs = plt.subplots(\n","    3, 2, figsize=(20, 12), tight_layout=True\n",")\n","\n","for i, leg in enumerate(legs):\n","    ax = axs.flatten()[i]\n","    for j, tarsalseg in enumerate(leg_contact_tarsalseg[leg]):\n","        ax.plot(output_t, leg_contacts[i, :, j], label=leg_contact_tarsalseg[leg][j])\n","    ax.set_xlabel(\"Time (s)\")\n","    ax.set_ylabel(\"Contact force\")\n","    if leg == \"LF\":\n","        ax.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\", borderaxespad=0)\n","\n","    ax.set_title(leg)"]},{"cell_type":"markdown","metadata":{"id":"TVn418XkKqY8"},"source":["## Tarsus angle"]},{"cell_type":"markdown","metadata":{"id":"8E5aES1hKqY8"},"source":["In our model, we do not enforce the position of the tarsal segments. Again due to the small size of the tarsal segments it is hard to measure their relative orientation. With Neuromechfly one can infer the angles between each tarsal segments and fill this gap."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":911},"executionInfo":{"elapsed":1602,"status":"ok","timestamp":1727422276476,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"9zQ_Nmx9KqY8","outputId":"f31d3d24-281a-488f-fd6e-af9251afb1cf"},"outputs":[],"source":["tarsus_joints_to_id = {leg: [i for i, joint in enumerate(monitored_joints) if leg in joint and \"Tarsus\" in joint] for leg in legs}\n","tarsus_jointangles = np.array([[obs[\"joints\"][0, tarsus_joints_to_id[leg]] for obs in obs_list] for leg in legs])\n","tarsus_joints = {leg: [joint for joint in monitored_joints if leg in joint and \"Tarsus\" in joint] for leg in legs}\n","\n","time = np.arange(len(leg_torques[0])) * timestep\n","\n","fig, axs = plt.subplots(\n","    3, 2, figsize=(20, 12), tight_layout=True\n",")\n","\n","for i, leg in enumerate(legs):\n","    ax = axs.flatten()[i]\n","    for j, joint in enumerate(tarsus_joints[leg]):\n","        ax.plot(output_t, np.rad2deg(tarsus_jointangles[i, :, j]), label=joint)\n","    ax.set_xlabel(\"Time (s)\")\n","    ax.set_ylabel(\"Torque (Nm)\")\n","    ax.set_title(leg)\n","    if leg == \"LF\":\n","        ax.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\", borderaxespad=0)\n"]},{"cell_type":"markdown","metadata":{"id":"dfIk2CKrKqY8"},"source":["## Vision"]},{"cell_type":"markdown","metadata":{"id":"pDX64uDtKqY8"},"source":["Neurmechfly v2 is largely oriented toward the modelling of higher order sensory modalities. With Neuromechfly v2 it is now possible to emulate the vision experienced by the fruitfly during behavior. This is again almost impossible to obtain without relying on kinematic relay."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":349},"executionInfo":{"elapsed":5,"status":"error","timestamp":1727422276476,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"4LU_DIkKKqZA","outputId":"fb723d55-8e9a-42c5-d7d7-fd472950e0e5"},"outputs":[],"source":["if fly.enable_vision:\n","    from flygym.vision.visualize import visualize_visual_input\n","    visualize_visual_input(\n","        fly.retina,\n","        output_dir/ \"retina_images.mp4\",\n","        [obs[\"vision\"] for obs in obs_list],\n","        raw_vision_list,\n","        fly.vision_update_mask[n_stabilization_steps:],\n","        playback_speed=play_speed,\n","    )\n","plt.close(\"all\")\n","\n","# play the video\n","Video(output_dir/ \"retina_images.mp4\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1727422276476,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"nYFXjW4xKqZA"},"outputs":[],"source":["n_imgs = 50\n","n_cols = 10\n","\n","fig, axs = plt.subplots(\n","    n_imgs // n_cols, n_cols, figsize=(20, 10), tight_layout=True\n",")\n","\n","selected_ids = np.linspace(0, len(obs_list), n_imgs, endpoint=False, dtype=int)\n","flat_ax = axs.flatten()\n","\n","for i, id in enumerate(selected_ids):\n","\n","    # subdivide each ax in two\n","    eyes = [fly.retina.hex_pxls_to_human_readable(obs_list[id][\"vision\"][i]).max(axis=2) for i in range(2)]\n","\n","    flat_ax[i].imshow(np.hstack(eyes), cmap=\"gray\")\n","    flat_ax[i].axis(\"off\")\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1727422276476,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"QrzPFoTgKqZA"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/NeLy-EPFL/neuromechfly-workshop/blob/main/kinematic_replay/kinematic_replay_colab.ipynb","timestamp":1727422496955}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"}},"nbformat":4,"nbformat_minor":0}
