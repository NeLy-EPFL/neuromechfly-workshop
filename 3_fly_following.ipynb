{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-fly Simulation and Visually Guided Fly Following\n",
    "\n",
    "In this notebook, we will demonstrate how one can simulate multiple flies in a single simulation, and how to implement an algorithm that allows a fly to follow another fly based on visual input.\n",
    "\n",
    "To run this notebook on Google Colab:\n",
    "- Locate the menu bar at the top of this page, go to \"Runtime\" → \"Change runtime type\", and select a GPU (e.g. \"T4 GPU\"). Save your setting.\n",
    "- Run the first code blocks.\n",
    "- Then, you can import FlyGym and start writing your own code.\n",
    "\n",
    "If you are not using Google Colab, the first code block won't do anything when executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "try:\n",
    "    import flygym\n",
    "\n",
    "    FLYGYM_INSTALLED = True\n",
    "except ImportError:\n",
    "    FLYGYM_INSTALLED = False\n",
    "\n",
    "if not FLYGYM_INSTALLED:\n",
    "    if IN_COLAB:\n",
    "        print(\n",
    "            \"I'm on Colab and FlyGym is not installed. I will try to install it now. \"\n",
    "            \"This will take a minute.\"\n",
    "        )\n",
    "        import subprocess\n",
    "\n",
    "        subprocess.run('pip install \"flygym[examples]\"', shell=True)\n",
    "    else:\n",
    "        print(\n",
    "            \"FlyGym is not installed, and I'm on your own computer. I can try to \"\n",
    "            \"install it here, but I don't want to modify your Python environment \"\n",
    "            \"unintentionally. Please install FlyGym yourself following instructions \"\n",
    "            \"from https://neuromechfly.org/installation.html\"\n",
    "        )\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"In Google Colab. I will now perform some Colab-specific setups.\")\n",
    "    # Set up GPU a few more and rendering parameters. This should take ~1 second.\n",
    "    from google.colab import files\n",
    "    import distutils.util\n",
    "    import os\n",
    "    import subprocess\n",
    "\n",
    "    if subprocess.run(\"nvidia-smi\").returncode:\n",
    "        raise RuntimeError(\n",
    "            \"Cannot communicate with GPU. \"\n",
    "            \"Make sure you are using a GPU Colab runtime. \"\n",
    "            \"Go to the Runtime menu and select Choose runtime type.\"\n",
    "        )\n",
    "\n",
    "    # Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
    "    # This is usually installed as part of an Nvidia driver package, but the Colab\n",
    "    # kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
    "    # (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
    "    NVIDIA_ICD_CONFIG_PATH = \"/usr/share/glvnd/egl_vendor.d/10_nvidia.json\"\n",
    "    if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
    "        with open(NVIDIA_ICD_CONFIG_PATH, \"w\") as f:\n",
    "            f.write(\n",
    "                \"\"\"{\n",
    "    \"file_format_version\" : \"1.0.0\",\n",
    "    \"ICD\" : {\n",
    "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "            )\n",
    "\n",
    "    # Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
    "    print(\"Setting environment variable to use GPU rendering:\")\n",
    "    %env MUJOCO_GL=egl\n",
    "\n",
    "    try:\n",
    "        print(\"Checking that the installation succeeded:\")\n",
    "        import mujoco\n",
    "\n",
    "        mujoco.MjModel.from_xml_string(\"<mujoco/>\")\n",
    "    except Exception as e:\n",
    "        raise e from RuntimeError(\n",
    "            \"Something went wrong during installation. Check the shell output above \"\n",
    "            \"for more information.\\n\"\n",
    "            \"If using a hosted Colab runtime, make sure you enable GPU acceleration \"\n",
    "            'by going to the Runtime menu and selecting \"Choose runtime type\".'\n",
    "        )\n",
    "\n",
    "    print(\"Installation successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"outputs/fly_following/\")\n",
    "output_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will simulate a virtual fly using vision to follow a target fly walking in a zig-zag pattern. First, let's explore how turning behavior can be implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple model of turning\n",
    "\n",
    "Basic CPGs can be implemented as feedforward networks of oscillators—in other words, the network behaves without taking into account sensory feedback. Similar to the formulation from [Ijspeert et al (2007)](https://doi.org/10.1126/science.1138353), the oscillator network can be described by the following ordinary differential equations (ODEs):\n",
    "$$ \\dot\\theta_i = 2\\pi\\nu_i + \\sum_{j} r_j w_{ij} \\sin(\\theta_j - \\theta_i - \\phi_{ij}) $$\n",
    "$$ \\dot r_i = \\alpha_i (R_i - r_i) $$\n",
    "where $\\theta_i$ and $r_i$ are the current phase and magnitude of the i-th oscillator. $R_i$ is the maximum amplitude of the i-th oscillator, and $\\alpha_i$ is a constant determining the rate of convergence to synchrony. $w_{ij}$ is the coupling weight between the i-th and the j-th oscillator, and $\\phi_{ij}$ is the phase bias between them. Intuitively, the first term of the first equation maintains an intrinsic frequency for each oscillator; the second term of the first equation keeps the oscillators synchronized (i.e., maintains the phase differences between the oscillators), and the second equation maintains the amplitudes of the oscillators.\n",
    "\n",
    "To achieve turning, we will use a 2-dimensional representation of descending signals $[\\delta_L, \\delta_R] \\in \\mathbb{R}^2$ to modulate the amplitude and direction of the leg CPGs on each side of the body. Specifically, we will modulate the intrinsic amplitude $R'$ and intrinsic frequency $\\nu'$ on each side by:\n",
    "\n",
    "$$\n",
    "R'(\\delta) = |\\delta|\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nu_i'(\\delta) = \\begin{cases}\n",
    "\\nu_i   & \\text{if } \\delta>0\\\\\n",
    "-\\nu_i  & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In other words, the magnitude of the descending signal controls the amplitude of stepping (as a gain applied to the originally recorded step size); the sign of the descending signal controls the direction of stepping. Although simplified, this model provides a basic framework for turning. Future improvements could address the assumption of linear step size scaling.\n",
    "\n",
    "The [flygym.examples.locomotion.HybridTurningFly](https://github.com/NeLy-EPFL/flygym/blob/main/flygym/examples/locomotion/turning_fly.py) class implements this model. In the `__init__` method of this class, a CPG network is created for controlling the leg movements:\n",
    "\n",
    "```Python\n",
    "        ...\n",
    "\n",
    "        # Initialize CPG network\n",
    "        self.cpg_network = CPGNetwork(\n",
    "                timestep=self.sim_params.timestep,\n",
    "                intrinsic_freqs=intrinsic_freqs,\n",
    "                intrinsic_amps=intrinsic_amps,\n",
    "                coupling_weights=coupling_weights,\n",
    "                phase_biases=phase_biases,\n",
    "                convergence_coefs=convergence_coefs,\n",
    "                seed=seed,\n",
    "        )\n",
    "        self.cpg_network.reset(init_phases, init_magnitudes)\n",
    "\n",
    "        ...\n",
    "```\n",
    "\n",
    "During each simulation step, the CPG’s intrinsic amplitudes and frequencies are updated according to the descending signal:\n",
    "\n",
    "```Python\n",
    "        amps = np.repeat(np.abs(action[:, np.newaxis]), 3, axis=1).ravel()\n",
    "        freqs = self.intrinsic_freqs.copy()\n",
    "        freqs[:3] *= 1 if action[0] > 0 else -1\n",
    "        freqs[3:] *= 1 if action[1] > 0 else -1\n",
    "        self.cpg_network.intrinsic_amps = amps\n",
    "        self.cpg_network.intrinsic_freqs = freqs\n",
    "```\n",
    "\n",
    "Next, these updated CPG phases and amplitudes are transformed into 7 joint angles for each of the six legs. This reduces the action space from 42 dimensions to just 2:\n",
    "\n",
    "```Python\n",
    "        for _ in range(n_steps):\n",
    "            obs, _, _, _, info = sim.step(np.array([0.2, 1.4]))     # instead of sim.step({\"joints\": joint_angles_42_dim})\n",
    "```\n",
    "\n",
    "The class also detects if the fly is stuck or stumbling, adjusting its leg movements accordingly. For more details, refer to the [source code](https://github.com/NeLy-EPFL/flygym/blob/main/flygym/examples/locomotion/turning_fly.py) of `HybridTurningFly` and the full tutorials on [hybrid controller](https://github.com/NeLy-EPFL/flygym/blob/main/notebooks/hybrid_controller.ipynb) and [hybrid turning controller](https://github.com/NeLy-EPFL/flygym/blob/main/notebooks/turning.ipynb).\n",
    "\n",
    "Now, let's test the fly's turning by applying an asymmetric descending signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flygym.arena import FlatTerrain\n",
    "from flygym import Camera, SingleFlySimulation\n",
    "from flygym.examples.locomotion import HybridTurningFly\n",
    "\n",
    "# TODO: Change the descending signal to something asymmetric,\n",
    "# which will cause the fly to turn:\n",
    "descending_signal = np.array([1, 1])\n",
    "###################################################################\n",
    "\n",
    "timestep = 1e-4\n",
    "contact_sensor_placements = [\n",
    "    f\"{leg}{segment}\"\n",
    "    for leg in [\"LF\", \"LM\", \"LH\", \"RF\", \"RM\", \"RH\"]\n",
    "    for segment in [\"Tibia\", \"Tarsus1\", \"Tarsus2\", \"Tarsus3\", \"Tarsus4\", \"Tarsus5\"]\n",
    "]\n",
    "\n",
    "fly = HybridTurningFly(\n",
    "    spawn_pos=(0, 0, 0.2),\n",
    "    contact_sensor_placements=contact_sensor_placements,\n",
    "    enable_adhesion=True,\n",
    ")\n",
    "cam = Camera(fly=fly, camera_id=\"Animat/camera_top\")\n",
    "arena = FlatTerrain(ground_alpha=0.1)\n",
    "sim = SingleFlySimulation(\n",
    "    fly=fly,\n",
    "    cameras=[cam],\n",
    "    arena=arena,\n",
    "    timestep=timestep,\n",
    ")\n",
    "obs, info = sim.reset(seed=0)\n",
    "obs_hist = []\n",
    "for _ in trange(4000):\n",
    "    obs, _, _, _, info = sim.step(descending_signal)\n",
    "    obs_hist.append(obs)\n",
    "    sim.render()\n",
    "\n",
    "cam.save_video(output_dir / \"turning.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [outputs/fly_following/turning.mp4](outputs/fly_following/turning.mp4) for the video output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Fly Simulation\n",
    "\n",
    "Next, we will simulate two flies: one standing still and the other walking in a zig-zag pattern. The zig-zagging fly alternates between left and right turns, which can be achieved by setting the left and right descending signals to out-of-phase square waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(0, 1.2, timestep)\n",
    "right_descending_signal = ((t - 0.2) % 0.8 > 0.4) * 1.2 + 0.2\n",
    "left_descending_signal = 1.6 - right_descending_signal\n",
    "descending_signals = np.column_stack([left_descending_signal, right_descending_signal])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 1.5), sharex=True)\n",
    "ax.plot(t, left_descending_signal, label=\"Left\", lw=2)\n",
    "ax.plot(t, right_descending_signal, label=\"Right\", linestyle=\"--\", lw=2)\n",
    "ax.legend(frameon=False, bbox_to_anchor=(1, 0.5), loc=\"center left\")\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_ylabel(\"Descending\\nsignal\", rotation=0, ha=\"center\", va=\"center\", labelpad=40)\n",
    "ax.set_xlim(0, 1.2)\n",
    "ax.set_ylim(0, 1.6)\n",
    "ax.set_yticks([0.2, 1.4])\n",
    "ax.set_xticks(np.arange(0, 1.4, 0.2))\n",
    "for spine in [\"top\", \"right\"]:\n",
    "    ax.spines[spine].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multi-fly simulations, we use the `Simulation` class instead of `SingleFlySimulation`. The Simulation class accepts a list of `Fly` objects, each of which can be an instance of any subclass of the `Fly` class. In this case, we’ll simulate two `HybridTurningFly` instances, which we’ll name \"Nuro\" and \"McFly.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flygym import Simulation\n",
    "\n",
    "nuro = HybridTurningFly(\n",
    "    name=\"Nuro\",\n",
    "    spawn_pos=(-5, 1, 0.2),\n",
    "    enable_adhesion=True,\n",
    "    enable_vision=True,\n",
    "    render_raw_vision=True,\n",
    "    contact_sensor_placements=contact_sensor_placements,\n",
    ")\n",
    "mcfly = HybridTurningFly(\n",
    "    name=\"McFly\",\n",
    "    spawn_pos=(0, 0, 0.2),\n",
    "    enable_adhesion=True,\n",
    "    contact_sensor_placements=contact_sensor_placements,\n",
    ")\n",
    "cam = Camera(fly=nuro, camera_id=\"Animat/camera_top_zoomout\")\n",
    "arena = FlatTerrain(ground_alpha=0.1)\n",
    "sim = Simulation(\n",
    "    flies=[nuro, mcfly],  # add both flies to the simulation\n",
    "    cameras=[cam],\n",
    "    arena=arena,\n",
    "    timestep=timestep,\n",
    ")\n",
    "obs, info = sim.reset(seed=0)\n",
    "\n",
    "obs_hist = []\n",
    "info_50 = None # info at the 50th time step for visualization\n",
    "\n",
    "for i, action_mcfly in enumerate(tqdm(descending_signals)):\n",
    "    obs, _, _, _, info = sim.step(\n",
    "        {\n",
    "            \"Nuro\": np.zeros(2),  # stand still\n",
    "            \"McFly\": action_mcfly,  # turn\n",
    "        }\n",
    "    )\n",
    "    sim.render()\n",
    "    if i == 50:\n",
    "        info_50 = info\n",
    "\n",
    "    obs_hist.append(obs)\n",
    "\n",
    "cam.save_video(output_dir / \"multifly.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [outputs/fly_following/multifly.mp4](outputs/fly_following/multifly.mp4) for the video output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multi-fly simulations, the action passed to the step method is a dictionary, with each fly's name as the key. Observations and info returned are also keyed by the flies' names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{obs.keys()=}\")\n",
    "print(f\"{obs['Nuro'].keys()=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Fly Trajectories\n",
    "Let's visualize the trajectories of both flies as Nuro watches McFly walking away:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for fly in sim.flies:\n",
    "    trajectory = np.array([i[fly.name][\"fly\"][0, :2] for i in obs_hist])\n",
    "    ax.plot(trajectory[:, 0], trajectory[:, 1], label=fly.name)\n",
    "\n",
    "ax.legend(\n",
    "    frameon=False, title=\"Trajectories\", bbox_to_anchor=(1, 0.5), loc=\"center left\"\n",
    ")\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_xlabel(\"x (mm)\")\n",
    "ax.set_ylabel(\"y (mm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make Nuro follows McFly based on vision. Since `enable_vision=True` was set for Nuro, its observations include `vision`, an ndarray containing light intensities (ranging from 0 to 1) detected by its ommatidia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"obs['Nuro']['vision'].shape={obs['Nuro']['vision'].shape}\")  # (number of retinas, number of ommatidia, number of channels)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These readings can be visualized using the `hex_pxls_to_human_readable` method of the `Retina` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the maximum value across all channels\n",
    "ommatidia_readings = obs_hist[50][\"Nuro\"][\"vision\"].max(axis=-1)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(6, 4), facecolor=\"k\")\n",
    "for i, (ax, eye) in enumerate(zip(axs, [\"Left\", \"Right\"])):\n",
    "    im = nuro.retina.hex_pxls_to_human_readable(ommatidia_readings[i])\n",
    "    ax.imshow(im, cmap=\"gray\", vmin=0, vmax=1)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"{eye} eye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As `enable_vision=True` was enabled during creation of Nuro, raw images (before conversion to ommatidia readings) are stored in the `raw_vision` field of the `info` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(6, 4), facecolor=\"k\")\n",
    "for i, (ax, eye) in enumerate(zip(axs, [\"Left\", \"Right\"])):\n",
    "    im = info_50[\"Nuro\"][\"raw_vision\"][i].astype(np.uint8)\n",
    "    ax.imshow(im, cmap=\"gray\", vmin=0, vmax=1)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"{eye} eye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Following Behavior\n",
    "We’ll now implement a simple following behavior. Nuro will compare the numbers of dimmed ommatidia on its left and right retinas. If there are more dimmed ommatidia on the left, Nuro will turn left, otherwise, it will turn right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ommatidia_readings = obs_hist[50][\"Nuro\"][\"vision\"].max(\n",
    "    axis=-1\n",
    ")  # take the maximum value across all channels\n",
    "threshold = 0.75\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(6, 4))\n",
    "for i, (ax, eye) in enumerate(zip(axs, [\"Left\", \"Right\"])):\n",
    "    im = nuro.retina.hex_pxls_to_human_readable(ommatidia_readings[i])\n",
    "    ax.imshow(im < threshold, cmap=\"gray\", vmin=0, vmax=1)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"{eye} eye\")\n",
    "\n",
    "ommatidia_l = (ommatidia_readings[0] < threshold).sum()\n",
    "ommatidia_r = (ommatidia_readings[1] < threshold).sum()\n",
    "\n",
    "fig.suptitle(\n",
    "    f\"L ({ommatidia_l}) {'<>'[int(ommatidia_l > ommatidia_l)]} R ({ommatidia_r}) -> turn {'RL'[int(ommatidia_l > ommatidia_l)]}\",\n",
    "    y=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuro = HybridTurningFly(\n",
    "    name=\"Nuro\",\n",
    "    contact_sensor_placements=contact_sensor_placements,\n",
    "    enable_adhesion=True,\n",
    "    spawn_pos=(-4.5, 0, 0.2),\n",
    "    enable_vision=True,\n",
    "    render_raw_vision=True,\n",
    ")\n",
    "mcfly = HybridTurningFly(\n",
    "    name=\"McFly\",\n",
    "    contact_sensor_placements=contact_sensor_placements,\n",
    "    enable_adhesion=True,\n",
    "    spawn_pos=(0, 0, 0.2),\n",
    ")\n",
    "cam = Camera(fly=nuro, camera_id=\"Animat/camera_top_zoomout\")\n",
    "arena = FlatTerrain(ground_alpha=0.1)\n",
    "sim = Simulation(\n",
    "    flies=[nuro, mcfly],\n",
    "    cameras=[cam],\n",
    "    arena=arena,\n",
    "    timestep=timestep,\n",
    ")\n",
    "obs, info = sim.reset(seed=0)\n",
    "\n",
    "obs_hist = []\n",
    "info_hist = []\n",
    "is_left = []\n",
    "\n",
    "action_nuro = np.zeros(2)\n",
    "decision_interval = 500\n",
    "\n",
    "for i, action_mcfly in enumerate(tqdm(descending_signals)):\n",
    "    ommatidia_readings = obs[\"Nuro\"][\"vision\"].max(-1)\n",
    "    activated_ommatidia = (ommatidia_readings < 0.5).sum(-1)\n",
    "    is_left.append(activated_ommatidia[0] > activated_ommatidia[1])\n",
    "\n",
    "    if i > 0 and i % decision_interval == 0:\n",
    "        # take the majority vote of the last `decision_interval` frames to\n",
    "        # avoid frequent changes due to shaky vision\n",
    "        if np.mean(is_left[-decision_interval:]) > 0.5:\n",
    "            action_nuro = np.array([0.2, 1.4])\n",
    "        else:\n",
    "            action_nuro = np.array([1.4, 0.2])\n",
    "\n",
    "    obs, _, _, _, info = sim.step({\"Nuro\": action_nuro, \"McFly\": action_mcfly})\n",
    "    sim.render()\n",
    "\n",
    "    obs_hist.append(obs)\n",
    "\n",
    "    # save memory by not storing frames when vision is not updated\n",
    "    if nuro.vision_update_mask[-1]:\n",
    "        info_hist.append(info)\n",
    "\n",
    "cam.save_video(output_dir / \"nuro_follows_mcfly.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [outputs/fly_following/nuro_follows_mcfly.mp4](outputs/fly_following/nuro_follows_mcfly.mp4) for the video output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the trajectories of both flies as Nuro follows McFly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for fly in sim.flies:\n",
    "    trajectory = np.array([i[fly.name][\"fly\"][0, :2] for i in obs_hist])\n",
    "    ax.plot(trajectory[:, 0], trajectory[:, 1], label=fly.name)\n",
    "\n",
    "ax.legend(\n",
    "    frameon=False, title=\"Trajectories\", bbox_to_anchor=(1, 0.5), loc=\"center left\"\n",
    ")\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_xlabel(\"x (mm)\")\n",
    "ax.set_ylabel(\"y (mm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can generate a video that shows Nuro's perspective as it follows McFly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flygym.vision import visualize_visual_input\n",
    "\n",
    "visualize_visual_input(\n",
    "    nuro.retina,\n",
    "    output_path=output_dir / \"nuro_follows_mcfly_retinal.mp4\",\n",
    "    vision_data_li=[obs[\"Nuro\"][\"vision\"] for i, obs in enumerate(obs_hist) if nuro.vision_update_mask[i]],\n",
    "    raw_vision_data_li=[info[\"Nuro\"][\"raw_vision\"] for info in info_hist],\n",
    "    vision_update_mask=np.ones(len(info_hist), dtype=bool),\n",
    "    vision_refresh_rate=nuro.vision_refresh_rate,\n",
    "    playback_speed=0.1,\n",
    ")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [outputs/fly_following/nuro_follows_mcfly_retinal.mp4](outputs/fly_following/nuro_follows_mcfly_retinal.mp4) for the video output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Interfacing NeuroMechFly with a connectome-constrained vision model\n",
    "\n",
    "Ultimately, to gain insights into the real workings of the biological controller, one would ideally build a controller with artificial neurons that can be mapped to neuron subtypes in the real fly nervous system. This can, in principle, be achieved by leveraging newly available brain and VNC connectomics datasets (see the [FlyWire](https://flywire.ai/) project for the brain, and the [FANC](https://connectomics.hms.harvard.edu/project1) and [MANC](https://www.janelia.org/project-team/flyem/manc-connectome) projects for the VNC).\n",
    "\n",
    "To illustrate how this might be accomplished, we will interface NeuroMechFly a recently established connectome-constrained neural network model ([Lappalainen et al., 2024](https://www.nature.com/articles/s41586-024-07939-3); [code](https://github.com/TuragaLab/flyvis)). This study has constructed an artificial neural network (ANN) representing the retina, lamina, medula, lobula plate, and lobula of the fly visual system (see figure below). The connectivity in this network is informed by the connectome and, unlike typical ANNs, models biologically meaningful variables such as voltage.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/NeLy-EPFL/_media/blob/main/flygym/advanced_vision/lappalainen_model_schematic.png?raw=true\" alt=\"lappalainen_model_schematic.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "_Image from Lappalainen et al., 2024._\n",
    "\n",
    "We will pass the visual experience of the simulated fly as inputs to this pretrained model and simulate the activities of real neurons. For this purpose, we have implemented a `RealisticVisionFly` class that extends `HybridTurningFly`. Let's initialize the simulation but replace the observer fly with an instance of `RealisticVisionFly`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flygym.examples.vision import RealisticVisionFly\n",
    "\n",
    "nuro = RealisticVisionFly(\n",
    "    name=\"Nuro\",\n",
    "    spawn_pos=(-5, 1, 0.2),\n",
    "    enable_adhesion=True,\n",
    "    contact_sensor_placements=contact_sensor_placements,\n",
    ")\n",
    "mcfly = HybridTurningFly(\n",
    "    name=\"McFly\",\n",
    "    spawn_pos=(0, 0, 0.2),\n",
    "    enable_adhesion=True,\n",
    "    contact_sensor_placements=contact_sensor_placements,\n",
    ")\n",
    "cam = Camera(fly=nuro, camera_id=\"Animat/camera_top_zoomout\")\n",
    "arena = FlatTerrain(ground_alpha=0.1)\n",
    "sim = Simulation(\n",
    "    flies=[nuro, mcfly],  # add both flies to the simulation\n",
    "    cameras=[cam],\n",
    "    arena=arena,\n",
    "    timestep=timestep,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit the scenario when Nuro stands still and McFly walks in a zig-zag pattern and see how different classes of neurons in the vision model respond to the moving fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = sim.reset(seed=0)\n",
    "viz_data_all = []\n",
    "obs_hist = []\n",
    "info_hist = []\n",
    "\n",
    "for action in tqdm(\n",
    "    descending_signals[:4000]\n",
    "):  # only simulate the first 4000 steps to save time\n",
    "    obs, _, _, _, info = sim.step(\n",
    "        {\n",
    "            \"Nuro\": np.zeros(2),  # stand still\n",
    "            \"McFly\": action,  # turn\n",
    "        }\n",
    "    )\n",
    "    obs_hist.append(obs)\n",
    "    rendered_img = sim.render()[0]\n",
    "\n",
    "    if rendered_img is not None:\n",
    "        viz_data = {\n",
    "            \"rendered_image\": rendered_img,\n",
    "            \"vision_observation\": obs[\"Nuro\"][\"vision\"],  # raw visual observation\n",
    "            \"nn_activities\": info[\"Nuro\"][\"nn_activities\"],  # neural activities\n",
    "        }\n",
    "        viz_data_all.append(viz_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flygym.examples.vision.viz import visualize_vision\n",
    "\n",
    "plt.rcdefaults()\n",
    "plt.ioff()\n",
    "\n",
    "visualize_vision(\n",
    "    video_path=output_dir / \"two_flies_walking_vision.mp4\",\n",
    "    retina=nuro.retina,\n",
    "    retina_mapper=nuro.retina_mapper,\n",
    "    viz_data_all=viz_data_all,\n",
    "    fps=cam.fps,\n",
    ")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [outputs/fly_following/two_flies_walking_vision.mp4](outputs/fly_following/two_flies_walking_vision.mp4) for the video output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the \"info\" dictionary, we can get the \"nn_activities\" entry, which is an extended dictionary containing the current activities of all neurons simulated in the network. For a complete definition of what the simulation returns in the observation and \"info\" dictionary, please refer to the [MDP Task Specification page](https://neuromechfly.org/api_ref/mdp_specs.html) of the API reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info[\"Nuro\"][\"nn_activities\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we can access the activities of the T4a/b/c/d neurons, which are known for encoding optical flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    1, 5, figsize=(6, 2), width_ratios=[2, 2, 2, 2, 0.2], tight_layout=True\n",
    ")\n",
    "\n",
    "for i, cell in enumerate([\"T4a\", \"T4b\", \"T4c\", \"T4d\"]):\n",
    "    ax = axs[i]\n",
    "\n",
    "    # Take the cell activities of the right eye (index 1)\n",
    "    cell_activities = info[\"Nuro\"][\"nn_activities\"][cell][1]\n",
    "    cell_activities = nuro.retina_mapper.flyvis_to_flygym(cell_activities)\n",
    "\n",
    "    # Convert the values of 721 cells to a 2D image\n",
    "    viz_img = nuro.retina.hex_pxls_to_human_readable(cell_activities)\n",
    "    viz_img[nuro.retina.ommatidia_id_map == 0] = np.nan\n",
    "    imshow_obj = ax.imshow(viz_img, cmap=\"seismic\", vmin=-2, vmax=2)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(cell)\n",
    "\n",
    "cbar = plt.colorbar(\n",
    "    imshow_obj,\n",
    "    cax=axs[4],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you think of a way to use the activities of the neurons (e.g., T4a/b/c/d) in the vision model to make Nuro follow McFly more effectively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flygym.examples.vision import RealisticVisionFly\n",
    "\n",
    "nuro = RealisticVisionFly(\n",
    "    name=\"Nuro\",\n",
    "    spawn_pos=(-5, 1, 0.2),\n",
    "    enable_adhesion=True,\n",
    "    contact_sensor_placements=contact_sensor_placements,\n",
    ")\n",
    "mcfly = HybridTurningFly(\n",
    "    name=\"McFly\",\n",
    "    spawn_pos=(0, 0, 0.2),\n",
    "    enable_adhesion=True,\n",
    "    contact_sensor_placements=contact_sensor_placements,\n",
    ")\n",
    "cam = Camera(fly=nuro, camera_id=\"Animat/camera_top_zoomout\")\n",
    "arena = FlatTerrain(ground_alpha=0.1)\n",
    "sim = Simulation(\n",
    "    flies=[nuro, mcfly],  # add both flies to the simulation\n",
    "    cameras=[cam],\n",
    "    arena=arena,\n",
    ")\n",
    "\n",
    "obs, info = sim.reset(seed=0)\n",
    "viz_data_all = []\n",
    "obs_hist = []\n",
    "\n",
    "for action_mcfly in tqdm(descending_signals):\n",
    "    # Modulate the descending signals with activities of neurons such as T4a/b/c/d\n",
    "    action_nuro = ...\n",
    "\n",
    "    obs, _, _, _, info = sim.step(\n",
    "        {\n",
    "            \"Nuro\": action_nuro,\n",
    "            \"McFly\": action_mcfly,\n",
    "        }\n",
    "    )\n",
    "    obs_hist.append(obs)\n",
    "\n",
    "    rendered_img = sim.render()[0]\n",
    "\n",
    "    if rendered_img is not None:\n",
    "        viz_data = {\n",
    "            \"rendered_image\": rendered_img,\n",
    "            \"vision_observation\": obs[\"Nuro\"][\"vision\"],  # raw visual observation\n",
    "            \"nn_activities\": info[\"Nuro\"][\"nn_activities\"],  # neural activities\n",
    "        }\n",
    "        viz_data_all.append(viz_data)\n",
    "\n",
    "cam.save_video(output_dir / \"realistic_vision_top_view.mp4\")\n",
    "\n",
    "plt.rcdefaults()\n",
    "plt.ioff()\n",
    "\n",
    "visualize_vision(\n",
    "    video_path=output_dir / \"realistic_vision_activities.mp4\",\n",
    "    retina=nuro.retina,\n",
    "    retina_mapper=nuro.retina_mapper,\n",
    "    viz_data_all=viz_data_all,\n",
    "    fps=cam.fps,\n",
    ")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [outputs/fly_following/realistic_vision_top_view.mp4](outputs/fly_following/realistic_vision_top_view.mp4) and [outputs/fly_following/realistic_vision_activities.mp4](outputs/fly_following/realistic_vision_activities.mp4) for the video outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flygym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
