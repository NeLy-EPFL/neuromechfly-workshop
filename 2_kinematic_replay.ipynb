{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kinematic Replay\n",
    "\n",
    "In this notebook, we will demonstrate how we can replay experimentally recorded fly kinematics to infer dynamical forces.\n",
    "\n",
    "To run this notebook on Google Colab:\n",
    "- Locate the menu bar at the top of this page, go to \"Runtime\" â†’ \"Change runtime type\", and select a GPU (e.g. \"T4 GPU\"). Save your setting.\n",
    "- Run the first code blocks.\n",
    "- Then, you can import FlyGym and start writing your own code.\n",
    "\n",
    "If you are not using Google Colab, the first code block won't do anything when executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11548,
     "status": "ok",
     "timestamp": 1727422208926,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "zoFKPNPGKqY1",
    "outputId": "43f83881-dee3-4d90-bb7e-1b543610fbab"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "try:\n",
    "    import flygym\n",
    "\n",
    "    FLYGYM_INSTALLED = True\n",
    "except ImportError:\n",
    "    FLYGYM_INSTALLED = False\n",
    "\n",
    "if not FLYGYM_INSTALLED:\n",
    "    if IN_COLAB:\n",
    "        print(\n",
    "            \"I'm on Colab and FlyGym is not installed. I will try to install it now. \"\n",
    "            \"This will take a minute.\"\n",
    "        )\n",
    "        import subprocess\n",
    "\n",
    "        subprocess.run('pip install \"flygym[examples]\"', shell=True)\n",
    "    else:\n",
    "        print(\n",
    "            \"FlyGym is not installed, and I'm on your own computer. I can try to \"\n",
    "            \"install it here, but I don't want to modify your Python environment \"\n",
    "            \"unintentionally. Please install FlyGym yourself following instructions \"\n",
    "            \"from https://neuromechfly.org/installation.html\"\n",
    "        )\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"In Google Colab. I will now perform some Colab-specific setups.\")\n",
    "    # Set up GPU a few more and rendering parameters. This should take ~1 second.\n",
    "    from google.colab import files\n",
    "    import distutils.util\n",
    "    import os\n",
    "    import subprocess\n",
    "\n",
    "    if subprocess.run(\"nvidia-smi\").returncode:\n",
    "        raise RuntimeError(\n",
    "            \"Cannot communicate with GPU. \"\n",
    "            \"Make sure you are using a GPU Colab runtime. \"\n",
    "            \"Go to the Runtime menu and select Choose runtime type.\"\n",
    "        )\n",
    "\n",
    "    # Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
    "    # This is usually installed as part of an Nvidia driver package, but the Colab\n",
    "    # kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
    "    # (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
    "    NVIDIA_ICD_CONFIG_PATH = \"/usr/share/glvnd/egl_vendor.d/10_nvidia.json\"\n",
    "    if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
    "        with open(NVIDIA_ICD_CONFIG_PATH, \"w\") as f:\n",
    "            f.write(\n",
    "                \"\"\"{\n",
    "    \"file_format_version\" : \"1.0.0\",\n",
    "    \"ICD\" : {\n",
    "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "            )\n",
    "\n",
    "    # Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
    "    print(\"Setting environment variable to use GPU rendering:\")\n",
    "    %env MUJOCO_GL=egl\n",
    "\n",
    "    try:\n",
    "        print(\"Checking that the installation succeeded:\")\n",
    "        import mujoco\n",
    "\n",
    "        mujoco.MjModel.from_xml_string(\"<mujoco/>\")\n",
    "    except Exception as e:\n",
    "        raise e from RuntimeError(\n",
    "            \"Something went wrong during installation. Check the shell output above \"\n",
    "            \"for more information.\\n\"\n",
    "            \"If using a hosted Colab runtime, make sure you enable GPU acceleration \"\n",
    "            'by going to the Runtime menu and selecting \"Choose runtime type\".'\n",
    "        )\n",
    "\n",
    "    print(\"Installation successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we have the necessary data and download it if we don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from hashlib import md5\n",
    "import urllib.request\n",
    "\n",
    "leg_joint_angles_path = Path(\"./data/inverse_kinematics/leg_joint_angles.pkl\")\n",
    "leg_joint_angles_expected_md5 = \"420d8380b2fcb9ca310f7936a11effd4\"\n",
    "leg_joint_angles_url = \"https://github.com/NeLy-EPFL/neuromechfly-workshop/raw/refs/heads/main/data/inverse_kinematics/leg_joint_angles.pkl\"\n",
    "\n",
    "\n",
    "# If file doesn't exist, we need to download it\n",
    "download_needed = not leg_joint_angles_path.is_file()\n",
    "\n",
    "# If file does exist, check if it's corrupted. If so, re-download it\n",
    "if not download_needed:\n",
    "    with open(\"./data/inverse_kinematics/leg_joint_angles.pkl\", \"rb\") as f:\n",
    "        data = f.read()\n",
    "        checksum = md5(data).hexdigest()\n",
    "        if checksum != leg_joint_angles_expected_md5:\n",
    "            download_needed = True\n",
    "\n",
    "if download_needed:\n",
    "    print(\"Downloading inverse kinematics data...\")\n",
    "    urllib.request.urlretrieve(leg_joint_angles_url, leg_joint_angles_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kinematic replay of untethered locomotion\n",
    "\n",
    "We now move on to an example where we perform kinematic replay of experimentally recorded walking behaviors. Specifically, we recorded an untethered fly walking in a narrow corridor. We used the GUI of SLEAP to manually annotate ~50 frames of behavior recorded at 360 fps and downsampled to 130fps for annotations. When then leveraged the simple geometrical constraints of our setup to triangulate the 2d poses to a 3d poses. Used df3d post processing for alignment and interpolation and finally used seqikpy to perform inverse kinematics (e.g calculate the angle at each DoF). All the notebooks used to go from a raw video to joint angles can be found in this repo in the folder: 2d-3d\n",
    "\n",
    "We will use a PD controller to actuate the DoFs of the simulated fly using these exact angles to see if the fly can walk untethered on flat terrain, as shown in the original NeuroMechFly paper (Lobato-Rios et al., *Nature Methods* 2022).\n",
    "\n",
    "Here is the above mentioned locomotor behavior we will replay.\n",
    "\n",
    "<video controls>\n",
    "  <source src=\"https://github.com/NeLy-EPFL/neuromechfly-workshop/raw/refs/heads/main/data/video_data/straight_walking_bout.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the previous notebook, we will perform the necessary import define some simulation parameters, format the data and visualize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data block preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2133,
     "status": "ok",
     "timestamp": 1727422211757,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "YxqMKcCbKqY5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import trange\n",
    "\n",
    "from flygym import Fly, Camera, SingleFlySimulation\n",
    "from flygym.preprogrammed import all_leg_dofs, all_tarsi_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = 1e-4\n",
    "actuated_joints = all_leg_dofs\n",
    "\n",
    "output_dir = Path(\"outputs/kinematic_replay/\")\n",
    "output_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-wIbkacKqY5"
   },
   "source": [
    "Check the format our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1727422211758,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "-bA9jfYnKqY5"
   },
   "outputs": [],
   "source": [
    "def format_seqikpy_data(\n",
    "    data,\n",
    "    corresp_dict={\"ThC\": \"Coxa\", \"CTr\": \"Femur\", \"FTi\": \"Tibia\", \"TiTa\": \"Tarsus1\"},\n",
    "):\n",
    "    data_gym = {}\n",
    "    for joint, values in data.items():\n",
    "        if joint == \"meta\" or joint == \"swing_stance_time\":\n",
    "            data_gym[joint] = values\n",
    "        else:\n",
    "            leg = joint[6:8]\n",
    "            joint_name = joint[9:]\n",
    "            seg, dof = joint_name.split(\"_\")\n",
    "            if dof == \"pitch\":\n",
    "                newjoint = f\"joint_{leg}{corresp_dict[seg]}\"\n",
    "            else:\n",
    "                newjoint = f\"joint_{leg}{corresp_dict[seg]}_{dof}\"\n",
    "\n",
    "            data_gym[newjoint] = values\n",
    "\n",
    "    return data_gym\n",
    "\n",
    "\n",
    "seq_ikpy_data_path = Path(leg_joint_angles_path)\n",
    "with open(seq_ikpy_data_path, \"rb\") as f:\n",
    "    seq_ikdata = pickle.load(f)\n",
    "\n",
    "data = format_seqikpy_data(seq_ikdata)\n",
    "\n",
    "data[\"meta\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1727422211758,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "6GtLA-juKqY6",
    "outputId": "addab902-a761-491d-83eb-2738e880148e"
   },
   "outputs": [],
   "source": [
    "run_time = len(data[\"joint_RFCoxa_yaw\"]) * data[\"meta\"][\"timestep\"]\n",
    "\n",
    "target_num_steps = int(run_time / timestep)\n",
    "data_block = np.zeros((len(actuated_joints), target_num_steps))\n",
    "input_t = np.arange(len(data[\"joint_LFCoxa\"])) * data[\"meta\"][\"timestep\"]\n",
    "output_t = np.arange(target_num_steps) * timestep\n",
    "for i, joint in enumerate(actuated_joints):\n",
    "    data_block[i, :] = np.interp(output_t, input_t, data[joint])\n",
    "\n",
    "print(\n",
    "    \"Neuromechfly has {} actuated joints and the data contains {} interpolated steps adding up to a toal of {} seconds\".format(\n",
    "        *data_block.shape, run_time\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1727422211758,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "HTGHF1jAKqY6"
   },
   "outputs": [],
   "source": [
    "# The fly should walk more on the tippy toes\n",
    "tarsus_offset = np.zeros(len(actuated_joints))\n",
    "for i, joint in enumerate(actuated_joints):\n",
    "    if \"Tarsus\" in joint:\n",
    "        data_block[i, :] = -1 * np.pi / 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "executionInfo": {
     "elapsed": 2686,
     "status": "ok",
     "timestamp": 1727422214442,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "zPWzLYgYKqY6",
    "outputId": "e6012b41-46c4-4a7d-8146-88a8d4dbebff"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(\n",
    "    3, 2, figsize=(8, 6), sharex=True, sharey=True, tight_layout=True\n",
    ")\n",
    "legs = [\n",
    "    f\"{side} {pos} leg\"\n",
    "    for pos in [\"front\", \"middle\", \"hind\"]\n",
    "    for side in [\"Left\", \"Right\"]\n",
    "]\n",
    "for i, leg in enumerate(legs):\n",
    "    ax = axs.flatten()[i]\n",
    "    leg_code = f\"{leg.split()[0][0]}{leg.split()[1][0]}\".upper()\n",
    "    for j, dof in enumerate(actuated_joints):\n",
    "        if dof.split(\"_\")[1][:2] != leg_code:\n",
    "            continue\n",
    "        ax.plot(output_t, np.rad2deg(data_block[j, :]), label=dof[8:])\n",
    "    ax.set_ylim(-180, 180)\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_ylabel(\"Angle (degree)\")\n",
    "    ax.set_yticks([-180, -90, 0, 90, 180])\n",
    "    ax.set_title(leg)\n",
    "    if leg == \"Right front leg\":\n",
    "        ax.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\", borderaxespad=0)\n",
    "\n",
    "fig.savefig(output_dir / \"single_step.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRAjF3qfKqY6"
   },
   "source": [
    "In each leg 7 degrees of freedom are actuated adding up to 6x7 actuated joints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qk-haWKeKqY7"
   },
   "source": [
    "# Run kinematic replay\n",
    "\n",
    "Lets focus on the heart of this notebook: kinematic replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27708,
     "status": "ok",
     "timestamp": 1727422437004,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "69AnrcA3KqY7",
    "outputId": "5464502a-9251-412c-b9cf-2383a6df2394"
   },
   "outputs": [],
   "source": [
    "# Monitor the tarsus passive joints\n",
    "tarsal_joints = [\n",
    "    \"_\".join([\"joint\", tarsus_seg])\n",
    "    for tarsus_seg in all_tarsi_links\n",
    "    if not \"Tarsus1\" in tarsus_seg\n",
    "]\n",
    "monitored_joints = actuated_joints + tarsal_joints\n",
    "\n",
    "# more info about those simulation parameters at neuromechfly.org\n",
    "fly = Fly(\n",
    "    init_pose=\"stretch\",\n",
    "    actuated_joints=actuated_joints,\n",
    "    control=\"position\",\n",
    "    monitored_joints=monitored_joints,\n",
    "    enable_vision=True,\n",
    "    render_raw_vision=True,\n",
    ")\n",
    "play_speed = 0.05\n",
    "# The camera parameters allows to visualize the magnitude and direction of the contact forces applied by the ground on the fly legs\n",
    "cam = Camera(\n",
    "    fly=fly,\n",
    "    camera_id=\"Animat/camera_left\",\n",
    "    play_speed=play_speed,\n",
    "    draw_contacts=True,\n",
    "    play_speed_text=True,\n",
    ")\n",
    "sim = SingleFlySimulation(\n",
    "    fly=fly,\n",
    "    cameras=[cam],\n",
    ")\n",
    "obs, info = sim.reset()\n",
    "\n",
    "obs_list = []\n",
    "raw_vision_list = []\n",
    "vision_list = []\n",
    "\n",
    "for i in trange(target_num_steps):\n",
    "    # here, we simply use the recorded joint angles as the target joint angles\n",
    "    obs, reward, terminated, truncated, info = sim.step({\"joints\": data_block[:, i]})\n",
    "\n",
    "    # reduce RAM usage by removing the vision data and storing it separately when rendered\n",
    "    vision = obs.pop(\"vision\")\n",
    "    obs_list.append(obs.copy())\n",
    "    if fly.render_raw_vision and fly._vision_update_mask[-1]:\n",
    "        raw_vision_list.append(info[\"raw_vision\"])\n",
    "        vision_list.append(vision)\n",
    "\n",
    "    sim.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171,
     "resources": {
      "http://localhost:8080/outputs/kinematic_replay_camera_left_contacts.mp4": {
       "data": "",
       "headers": [
        [
         "content-length",
         "0"
        ]
       ],
       "ok": false,
       "status": 404,
       "status_text": ""
      }
     }
    },
    "executionInfo": {
     "elapsed": 2668,
     "status": "ok",
     "timestamp": 1727422449606,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "veoqmOquKqY7",
    "outputId": "e96472ad-39a4-4def-81ec-5f338bb25982"
   },
   "outputs": [],
   "source": [
    "video_name = \"kinematic_replay_{}.mp4\".format(cam.camera_id.split(\"/\")[1])\n",
    "if cam.draw_contacts:\n",
    "    video_name = video_name.replace(\".mp4\", \"_contacts.mp4\")\n",
    "\n",
    "output_path = output_dir / video_name\n",
    "cam.save_video(output_path, stabilization_time=0)\n",
    "\n",
    "\n",
    "mp4 = open(output_path, \"rb\").read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<video controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\"\n",
    "    % data_url\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQx2ouCWKqY7"
   },
   "source": [
    "The first output of kinematic replay is a video that can be rendered from any different angles. Arrows at the leg tips represent the contact forces with the floor with blue being forces along the z axis, green arrows represent forces along the y axis and red arrows along the x axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5Fk9uj8KqY7"
   },
   "source": [
    "## Observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1727422449606,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "nnJ6-TQSKqY7",
    "outputId": "82d28890-aafc-41be-f153-1c02757d6704"
   },
   "outputs": [],
   "source": [
    "print(f\"Observation list made {len(obs_list)} observations\")\n",
    "print(\"One observation contains information about:\")\n",
    "for key in obs_list[0].keys():\n",
    "    try:\n",
    "        print(key, obs[key].shape)\n",
    "    except AttributeError:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each observation contains the following information:\n",
    "\n",
    "Joints: Data structured as a (3, 66) array. For the 66 monitored joints, it monitors the angle, angular velocity and acceleration of joint.\n",
    "Fly: A (4, 3) array that describes the fly's position, velocity, rotation, and rotational velocity.\n",
    "Contact Forces: A (30, 3) array that captures the 3D contact forces at each of the fly's 30 contact points (e.g 6 legs with 5 tarsal segments per leg).\n",
    "End Effectors: A (6, 3) array representing the position of the fly's six end effectors.\n",
    "Fly Orientation: A (3,) array containing a vector monitoring the long axis of the Thorax (e.g antero-posterior axis of the fly)\n",
    "Cardinal Vectors: A (3, 3) array that contains the cardinal vectors aligned with the anterio-posterior, left-right and dorso-ventral axis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring NeuromechFly's Observation Space and it's links to *Drosophila melanogasters* sensory capabilities\n",
    "\n",
    "In this notebook, we eÂ£xplore how the observation space in **NeuromechFly** can be used to simulate different sensory organs found in the fly. Specifically, we focus on two important sensory systems: **campaniform sensilla** and **vision**. Of course reproducing the exact response of those sensory modalities is far beyond the scope of your model and would requires hours of modelling work. Nevertheless, by using the observation data from NeuromechFly, we can get a first approximation of the response of these organs during the replayed behavior. \n",
    "\n",
    "In the following sections, we will explore how the observation space can be extracted and plotted and how it can relate to the response of the *Drosophila's* sensory organs:\n",
    "- **Campaniform Sensilla**\n",
    "- **Vision**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Campaniform Sensilla\n",
    "\n",
    "### Overview\n",
    "**Campaniform sensilla** are mechanoreceptors found on the legs, antennas, halteres and wings of flies that detect mechanical strain or stress on the cuticle. These sensors help the fly monitor forces acting on its legs during walking or flying. The neurons associated with campaniform sensilla respond when the cuticle is deformed due to external forces, providing proprioceptive feedback to control movement.\n",
    "\n",
    "- **Anatomy**: These sensory organs are typically located in clusters on the legs, especially near the joints. Each sensillum consists of a dome-shaped cuticle that responds to strain, compressing and stimulating sensory neurons beneath it.\n",
    "- **Sensory function**: Campaniform sensilla detect strain forces in the cuticule. In our model and in the biological fly, sources of cuticule strain are interactions with the environemnet and forces applied by the muscles. In **NeuromechFly** this strain could be approximated using a probably nonlinear combination of **contact forces** and **joint torques** from the observation space.\n",
    "\n",
    "### Figure: Illustration of campaniform sensilla on a cockroach's tibia \n",
    "\n",
    "<figure>\n",
    "    <img\n",
    "    src=\"https://github.com/NeLy-EPFL/_media/blob/main/flygym/kinematic_replay/campanoform_sensilla.jpeg?raw=true\"\n",
    "    alt=\"rule_based\"\n",
    "    width=\"800\"\n",
    "    style=\"width: 510px; height: 337px; object-fit: cover; object-position: 0% 0;\"\n",
    "    />\n",
    "    <figcaption>source: Tuthill, J. C., & Wilson, R. I. (2016). Mechanosensation and adaptive motor control in insects. Current Biology </figcaption>\n",
    "\n",
    "</figure>\n",
    "\n",
    "\n",
    "**References**:\n",
    "- Tuthill, J. C., & Wilson, R. I. (2016). Mechanosensation and adaptive motor control in insects. Current Biology, 26(20), R1022-R1038.\n",
    "- Zill, S., Schmitz, J., & BÃ¼schges, A. (2004). Load sensing and control of posture and locomotion. Arthropod structure & development, 33(3), 273-286.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfqvDD_XKqY8"
   },
   "source": [
    "### Ground reaction forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4791,
     "status": "ok",
     "timestamp": 1727422274878,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "n1NSpNeTKqY8",
    "outputId": "01964257-74b6-492b-f641-bf7910a5a312"
   },
   "outputs": [],
   "source": [
    "legs = [side + pos for pos in \"FMH\" for side in \"LR\"]\n",
    "\n",
    "# For each leg get the indices of the tarsal segments\n",
    "leg_tarsal_seg_contact_id = {\n",
    "    leg: [i for i, tarsal_seg in enumerate(all_tarsi_links) if leg in tarsal_seg]\n",
    "    for leg in legs\n",
    "}\n",
    "# For each get the norm of the sum of the contact forces on the tarsal segments\n",
    "leg_contacts = np.linalg.norm(\n",
    "    [\n",
    "        np.sum(\n",
    "            [\n",
    "                [obs[\"contact_forces\"][i] for obs in obs_list]\n",
    "                for i in leg_tarsal_seg_contact_id[leg]\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "        for leg in legs\n",
    "    ],\n",
    "    axis=-1,\n",
    ")\n",
    "time = np.arange(len(leg_contacts[0])) * timestep\n",
    "\n",
    "fig, axs = plt.subplots(len(legs), 1, figsize=(10, 10), tight_layout=True, sharey=True)\n",
    "colors = plt.get_cmap(\"tab10\", len(legs))\n",
    "for i, leg in enumerate(legs):\n",
    "    ax = axs[i]\n",
    "    ax.plot(time, leg_contacts[i], label=leg, color=colors(i))\n",
    "    ax.set_ylabel(\"Contact force [ÂµN]\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    ax.set_ylim(0, 10)\n",
    "\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "fig.savefig(output_dir / \"ground_contact_forces.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: In mujoco no units are imposed, consistency is the key. Lets walk through those. Units of distance are given in **mm** in our simulation, units of mass are given in **g** and units of time in **s**.\n",
    "\n",
    "Hence the magnitude of the gravitational acceleration should be set to -98100 **$\\frac{mm}{s^2}$**. \n",
    "\n",
    "Force are given in **$\\frac{mm g}{s^2}$** equivalent to **$ÂµN$**: micro Newtons.\n",
    "\n",
    "Torques are given in **$ÂµN mm$**.\n",
    "\n",
    "All physical quantities can be derived in this way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u82QUvXNKqY7"
   },
   "source": [
    "### Joint torques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will track the torque of all joints in a single leg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 911
    },
    "executionInfo": {
     "elapsed": 2403,
     "status": "ok",
     "timestamp": 1727422453585,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "Vbu3YXVlKqY7",
    "outputId": "90d9a423-351e-46aa-be91-a3ec5f70f82d"
   },
   "outputs": [],
   "source": [
    "# Get indices corresponding to joints of the same leg\n",
    "leg_joints_to_id = {\n",
    "    leg: [i for i, joint in enumerate(monitored_joints) if leg in joint] for leg in legs\n",
    "}\n",
    "focus_leg = \"LH\"\n",
    "# Extract torque values (index 2 corresponds to torque)\n",
    "leg_torques = (\n",
    "    np.array([obs[\"joints\"][2, leg_joints_to_id[focus_leg]] for obs in obs_list]) * 1e9\n",
    ")\n",
    "# note the multiplication by 1e9 to convert to ÂµN.mm (our implementation converted the torque to N.m but for consistency with the contact forces we convert it to ÂµN.mm)\n",
    "\n",
    "leg_joints = [joint for joint in monitored_joints if focus_leg in joint]\n",
    "\n",
    "# Create subplots with shared x-axis, no overlapping y-labels, and tight layout\n",
    "fig, axs = plt.subplots(\n",
    "    len(leg_joints),\n",
    "    1,\n",
    "    figsize=(12, 10),\n",
    "    sharex=True,\n",
    "    gridspec_kw={\"hspace\": 0.5},\n",
    "    tight_layout=True,\n",
    ")\n",
    "\n",
    "# Colors for the joints (tab10 colormap)\n",
    "colors = plt.get_cmap(\"viridis\", len(leg_joints))\n",
    "\n",
    "# Loop through each joint and plot its torque data\n",
    "for i, (ax, joint) in enumerate(zip(axs, leg_joints)):\n",
    "    # Check if it's a passive or active joint\n",
    "    label = joint.replace(\"joint_\" + focus_leg, \"\")\n",
    "    if not (\"_\" in label):\n",
    "        label = label + \"_pitch\"\n",
    "    if \"Tarsus\" in label and not \"Tarsus1\" in label:\n",
    "        linestyle = \"dashed\"  # Passive joints (tarsal) use dotted lines\n",
    "    else:\n",
    "        linestyle = \"solid\"  # Active joints use solid lines\n",
    "\n",
    "    # Plot the torque data with appropriate linestyle\n",
    "    ax.plot(time, leg_torques[:, i], color=colors(i), linestyle=linestyle, label=label)\n",
    "\n",
    "    # Remove the box around the plot (spines)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(True)\n",
    "    ax.spines[\"bottom\"].set_visible(True)\n",
    "    ax.set_ylabel(joint.replace(\"joint_\", \"\")[2:])\n",
    "\n",
    "    # Disable y-tick labels to save space\n",
    "    ax.tick_params(axis=\"y\", which=\"both\", left=True, right=False)\n",
    "\n",
    "# Add a single y-axis label for torque (centered on the figure)\n",
    "fig.text(0.04, 0.5, \"Torque [ÂµN.mm]\", va=\"center\", rotation=\"vertical\", fontsize=10)\n",
    "\n",
    "# Set the x-axis label on the last subplot\n",
    "axs[-1].set_xlabel(\"Time (s)\", fontsize=10)\n",
    "axs[-1].tick_params(axis=\"x\", labelsize=8)  # Smaller tick labels for x-axis\n",
    "\n",
    "# Add a legend (outside the plot)\n",
    "handles, labels = [], []\n",
    "# add invisible black full and dashed lines to the legend to indicate active and passive joints\n",
    "handles.append(plt.Line2D([0], [0], color=\"black\", linestyle=\"solid\", linewidth=1))\n",
    "labels.append(\"Active joint\")\n",
    "handles.append(plt.Line2D([0], [0], color=\"black\", linestyle=\"dashed\", linewidth=1))\n",
    "labels.append(\"Passive joint\")\n",
    "# add the actual joint labels\n",
    "fig.legend(handles, labels, loc=\"upper right\", bbox_to_anchor=(0.95, 0.95))\n",
    "\n",
    "fig.suptitle(f\"Torque values for {focus_leg} leg joints\", fontsize=12)\n",
    "fig.savefig(output_dir / \"joint_torques.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfIk2CKrKqY8"
   },
   "source": [
    "## 2. Vision\n",
    "\n",
    "A flyâ€™s compound eye consists of âˆ¼700â€“750 individual units called ommatidia arranged in a hexagonal pattern (see the left panel of the figure below from the [droso4schools project](https://droso4schools.wordpress.com/l4-enzymes/#5); see also [this article](https://azretina.sites.arizona.edu/node/789) from the Arizona Retina Project). To emulate this, we attached a color camera to each of our modelâ€™s compound eyes (top right panel). We then transformed each camera image into 721 bins, representing ommatidia. Based on previous studies, we assume a 270Â° combined azimuth for the flyâ€™s field of view, with a âˆ¼17Â° binocular overlap. Visual sensitivity has evolved to highlight ethologically relevant color spectra at different locations in the environment. Here, as an initial step toward enabling this heterogeneity in our model, we implemented yellow- and pale-type ommatidiaâ€”sensitive to the green and blue channels of images rendered by the physics simulatorâ€”randomly assigned at a 7:3 ratio (as reported in [Rister et al, 2013](https://pubmed.ncbi.nlm.nih.gov/23293281/)). Users can substitute the green and blue channel values with the desired light intensities sensed by yellow- and pale-type ommatidia to achieve more biorealistic chromatic vision.\n",
    "\n",
    "<img src=\"https://github.com/NeLy-EPFL/_media/blob/main/flygym/vision_basics/vision.png?raw=true\" alt=\"rule_based\" width=\"800\"/>\n",
    "\n",
    "Neurmechfly v2 is largely oriented toward the modelling of higher order sensory modalities. With Neuromechfly v2 it is now possible to emulate the vision experienced by the fruitfly during behavior. This is again almost impossible to obtain without relying on kinematic relay.\n",
    "\n",
    "Here is a representation of both the raw output of the cameras as well as the fly's simulated vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "error",
     "timestamp": 1727422276476,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "4LU_DIkKKqZA",
    "outputId": "fb723d55-8e9a-42c5-d7d7-fd472950e0e5"
   },
   "outputs": [],
   "source": [
    "from flygym.vision.visualize import visualize_visual_input\n",
    "\n",
    "visualize_visual_input(\n",
    "    fly.retina,\n",
    "    output_dir / \"retina_images.mp4\",\n",
    "    vision_list,\n",
    "    raw_vision_list,\n",
    "    np.ones(\n",
    "        len(raw_vision_list), dtype=bool\n",
    "    ),  # as we only saved the raw vision when rendered we set all to True\n",
    "    playback_speed=play_speed,\n",
    ")\n",
    "plt.close(\"all\")\n",
    "\n",
    "# play the video\n",
    "mp4 = open(output_dir / \"retina_images.mp4\", \"rb\").read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<video controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\"\n",
    "    % data_url\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://github.com/NeLy-EPFL/neuromechfly-workshop/blob/main/kinematic_replay/kinematic_replay_colab.ipynb",
     "timestamp": 1727422496955
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
